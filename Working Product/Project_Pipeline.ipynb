{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9dccd1a",
   "metadata": {},
   "source": [
    "# Latent News Probability Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01049288",
   "metadata": {},
   "source": [
    "Directly below contains the pip installs that are required for this project. If you run this code and still don't have the required packages, you may need to add a `!pip install [library_name]` into this code block. This does not need to be run every time the model is used. P.S it might be better to run one pip install command at a time, because they can take a long time to run.\n",
    "\n",
    "IMPORTANT: If you are running this for the first time you may need to download these packages before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe and sentiment analysis\n",
    "!pip install pandas\n",
    "!pip install spacy\n",
    "!pip install spacytextblob\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Webscraping\n",
    "!pip install newspaper3k\n",
    "\n",
    "# Topic modeling \n",
    "!pip install gensim\n",
    "\n",
    "# Data vis\n",
    "!pip install plotly\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install wordcloud\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a2232",
   "metadata": {},
   "source": [
    "Spacy: Used for NLP and has the machine learning module\n",
    "    \n",
    "SpacyTextBlob: Used for the sentiment analysis\n",
    "    \n",
    "Pandas: Stores the data as a dataframe table\n",
    "    \n",
    "NewsPaper: Used for web scraping\n",
    "\n",
    "Gensim: Used for topic modeling\n",
    "    \n",
    "Requests: Makes the connection to the URL\n",
    "\n",
    "Plotly: Used for data visualization\n",
    "\n",
    "Matplotlib: Used for clustering and data visualization\n",
    "\n",
    "Sklearn: Used for clustering and data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1471074",
   "metadata": {},
   "source": [
    "## Start Here: Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97171282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newspaper3k web scraping\n",
    "from newspaper import Article\n",
    "\n",
    "from random import shuffle\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import glob\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary  # Import the Dictionary class from Gensim\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import plotly.express as px\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# K-Means Clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Principled Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38b484",
   "metadata": {},
   "source": [
    "IMPORTANT: Run all helper files. The function logic is stored here, and sorted by the part of the pipeline they support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89933f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SentimentAnalysis.ipynb\n",
    "%run WebScraper.ipynb\n",
    "%run PipelineHelpers.ipynb\n",
    "%run TopicModeling.ipynb\n",
    "%run DataVisualization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767401a",
   "metadata": {},
   "source": [
    "# General Pipeline Settings, and Pre-Analysis Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afdeff",
   "metadata": {},
   "source": [
    "Below are the customizable settings for all of the analysis and processing parts of the pipeline.\n",
    "\n",
    "Rerun this any time you make changes to these settings, as they will affect the outcome of certain pipeline steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787c3f5",
   "metadata": {},
   "source": [
    "IMPORTANT: The csvFile object should be set to a properly setup csv file of article URLs that you copy-paste into the same directory as the pipeline files (we use the relative path to access the file). The csvFile format is explained in the user manual, but simply it needs one column with the first entry being \"Address\" and the other entries all being article URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e23034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### BELOW IS OUR CUSTOMIZABLE SETTINGS\n",
    "###\n",
    "\n",
    "# This is the CSV file that is read in\n",
    "# This csv file should contain a single column of article urls, with the first entry being \"Address\"\n",
    "csv_file = \"urls_gabriel_generated.csv\"\n",
    "\n",
    "\n",
    "# WEBSCRAPING SETTINGS\n",
    "# This is the minimum words that we allow for an article\n",
    "# (to prevent certain blocked articles or bad data)\n",
    "word_count_filter = 150\n",
    "\n",
    "# This is the minimum amount of repeated phrases in a text that we will throw an error for\n",
    "repeated_phrase_filter = 25\n",
    "\n",
    "# This is the list of all social media URLs that we BLOCK (anything in this list WILL NOT be scraped)\n",
    "social_starts_with = [\"https://www.youtube.com\", \"https://youtu.be\", \"https://www.facebook.com\",\n",
    "                          \"https://twitter.com\", \"https://gettr.com/\"]\n",
    "\n",
    "# When webscraping, will only grab the first url_max URLs (set to -1 to turn off)\n",
    "url_max = 50\n",
    "\n",
    "# Used to turn on and off the adjusted labels\n",
    "label_adjuster_on = True\n",
    "\n",
    "# TOPIC MODELING SETTINGS\n",
    "# this will set how our topic modeling is generated: limit to the # of topics,\n",
    "# # of topics we start at, how many we increment by every step\n",
    "topic_model_dict = {\"topic_limit\": 5, \"topic_start\": 1, \"topic_step\": 2}\n",
    "\n",
    "\n",
    "###\n",
    "### END OF CUSTOMIZABLE SETTINGS\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fabe01",
   "metadata": {},
   "source": [
    "# MAIN PIPELINE START"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52c758",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f184f9",
   "metadata": {},
   "source": [
    "Below is the start of our pipeline.\n",
    "\n",
    "Here we create our nlp object and add the pipe to it for sentiment analysis. We then read in our .csv file and convert it to a dataframe to pass in to our sentiment analysis function, and gathers the list of all URLs.\n",
    "\n",
    "Our main_pipeline_sentiment_analysis() function takes in the list of urls, web scrapes their text, and performs sentiment analysis on them, creating a dataframe of all properly scraped articles and their associated sentiment analysis data.\n",
    "\n",
    "df is the main dataframe object we store our data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae169b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading the nlp pipeline and adding spacytextblob\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob') #This is used in our sentiment analysis\n",
    "\n",
    "# Using panda to load in our .csv file\n",
    "# File we are checking\n",
    "df = pd.read_csv(csv_file)\n",
    "# Column name we are checking (You can switch this column name to match the csv file)\n",
    "urls = df[\"Address\"].tolist()\n",
    "\n",
    "# Trim to only include the first url_max URLs\n",
    "if url_max > 0:\n",
    "    urls = urls[0:url_max]\n",
    "\n",
    "# Here is where we start the sentiment analysis over all articles\n",
    "df = main_pipeline_sentiment_analysis(urls)\n",
    "\n",
    "if label_adjuster_on:\n",
    "    df = label_adjuster(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b94a9",
   "metadata": {},
   "source": [
    "# TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eba53b",
   "metadata": {},
   "source": [
    "# Create Topic Model w/ Highest Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f58b3",
   "metadata": {},
   "source": [
    "Here we create our LDA topic model, article corpus, and topic-level sentiment analysis dictionary.\n",
    "\n",
    "First we pass in our dataframe df and associated settings. The function will create topic models with varying number of topics, compute the topic coherence score for that number of topics over our articles, and return to us the LDA model that has the highest coherence score. It will display a small graphic showing the topic coherence for each number of topics we tested.\n",
    "\n",
    "Next we will create our topic-level sentiment analysis dictionary using both our dataframe df and the new LDA_model and corpus we just created. This will give us a dictionary of the averaged sentiment scores for every topic of an article, for all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b11718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate multiple topic models with various topic count, then return\n",
    "# the model with highest coherence it will also display a quick visual of\n",
    "# coherence values, to understand what topic count is best and why it was chosen\n",
    "lda_model, corpus = create_lda_model(topic_model_dict[\"topic_limit\"],\n",
    "                                     topic_model_dict[\"topic_start\"], \n",
    "                                     topic_model_dict[\"topic_step\"])\n",
    "\n",
    "# lda_model is the lda model we will work with, corpus is the actual corpus of articles we use\n",
    "\n",
    "\n",
    "# Here we are making our dictionary of all articles and their associated\n",
    "# topic-level sentiment for every topic\n",
    "topic_level_sentiment = topic_sentence_sentiment_analysis(df, lda_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c4670",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf78edd",
   "metadata": {},
   "source": [
    "## Pre-processing dataframe for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f34a83",
   "metadata": {},
   "source": [
    "Here we preprocess our dataframe to add in a few relevant columns for data vis, as well as creating our topic relevancy dataframe and weighted sentiment dataframe over all topics for all articles. \n",
    "\n",
    "We add the Topics, Main Topic, Main Topic Score, and Shortened Address to the main dataframe (using our preprocess_dataframe_for_datavis() function). This lets the data vis section easily grab the information.\n",
    "\n",
    "Next we create the topic relevancy dataframe df_topics. In df_topics, each row is an article, and each column is a topic. For an article, every cell value represents the relevancy of that topic to that article. So if row 0 column 1 is 0.998, that means Article 0 has a 0.998 relevancy score on Topic 1.\n",
    "\n",
    "We create the per_topic_df, which uses our topic-based sentiment dictionary and our topic relevancy dataframe to make a dataframe where each row is an article and each column is a topic. The cell contains data of the form (sentiment, relevancy) which is a tuple that describes the sentiment value and topic relevancy of that article (row) for that topic (column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "097d2461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This generates our dataframe of per-topic sentiment and relevancy for all articles\n",
    "df_topics = generate_topic_relevancy_dataframe(lda_model)\n",
    "\n",
    "# Generate our per-topic dataframe\n",
    "# Format is (SENTIMENT, RELEVANCY)\n",
    "per_topic_df = create_per_topic_tuple_df(topic_level_sentiment, df_topics)\n",
    "\n",
    "# This generates a dataframe ready to use with our data visualizations\n",
    "# (includes some extra information)\n",
    "# This sorts our dataframe so must go last\n",
    "df = preprocess_dataframe_for_datavis(df, lda_model, corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d0141",
   "metadata": {},
   "source": [
    "## Data Visualization Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a648113",
   "metadata": {},
   "source": [
    "Here are the customizable settings for our data visualization section. Since these settings only affect how the data will be visualized, you can rerun the pipeline from this point down without rerunning any of the previous steps. This is recommended if you want to see different parameters for the settings visualized on the same data and LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fffb7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA VISUALIZATION SETTINGS\n",
    "# (starts at topic 0) set this list to every topic number you want\n",
    "# displayed individually\n",
    "list_of_topics_to_visualize = [8,9]\n",
    "\n",
    "# This is the settings dictionary for our k-means cluster generation.\n",
    "# max_clusters is used in our inertia visualization,\n",
    "# While num_clusters is our actual cluster count to use and pca_components\n",
    "# is the PCA component count that we use in both (2 should be used because we reduce to 2 dimensions)\n",
    "# the visualization of inertia as well as our actual clustering model.\n",
    "kmeans_settings = {\"max_clusters\": 10, \"num_clusters\": 4, \"pca_components\": 2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e1d98",
   "metadata": {},
   "source": [
    "## Display the Main Dataframe and Display the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef6a4c",
   "metadata": {},
   "source": [
    "Display the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the main dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcda24f",
   "metadata": {},
   "source": [
    "Display the topic model words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd35257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionary of all topic words\n",
    "topic_model_display = create_topic_words_dict(lda_model)\n",
    "\n",
    "# Display dictionary of all topics and their topic words\n",
    "topic_model_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587fb26",
   "metadata": {},
   "source": [
    "## Visualize all articles on their main topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e350ce8",
   "metadata": {},
   "source": [
    "Here we will show two simple graphs plotting all articles' sentiment values, with the articles sorted by main topic. This is to show the distribution of sentiment towards the different topics. The scatter plot will let you look each article individually, to see how each article contributes to the sentiment distribution. The box plot will show you the actual sentiment distribution for each topic, andgive details on the variance and average sentiment of a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d1cda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_all_articles_on_main_topic(df)\n",
    "visualize_all_articles_on_main_topic(df, plot_type=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dceebf",
   "metadata": {},
   "source": [
    "## Generating 2D and 3D Cluster Graphs of Topics (T-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fc21b",
   "metadata": {},
   "source": [
    "This visualization generates a 2D clustering graph representing each article as a point in the topic space, which uses t-SNE dimensionality reduction and clustering. The goal of this visualization is to show every article compared to each other by their similarity in topics. You should see clusters of articles that share a lot of topic words and overall themes, while articles that differ in their topics are shown far apart. Colored by an article's main topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235fbeca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_topic_cluster_tsne(lda_model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fea08",
   "metadata": {},
   "source": [
    "This generates a 3D clustering graph where the x and y-axis represent each article in the topic space, and the z-axis shows the sentiment value for each article. The 2D xy-plane is exactly the same as the 2D graph generated above, but we add a 3rd dimension in the form of sentiment to show how article clusters differ by their sentiment value as well as their topics. Colored by main topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_topic_cluster_tsne_3d(lda_model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a452f5",
   "metadata": {},
   "source": [
    "## Subjectivity vs Sentiment of Articles for a single topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b721a6",
   "metadata": {},
   "source": [
    "Will iterate through our list_of_topics_to_visualize and will make a graph for each topic number in that list as well as that topic's word cloud of the top 10 topic words for that topic. So for each topic we want to look at, it will show all the articles with that topic as their main topic, and plot the articles on their subjectivity vs sentiment. A higher subjectivity will show you that the author chose more opinionated words, rather than relying on neutral, factual statements. Compare that with the sentiment value to show whether they are positive and negative towards that topic, and how factual vs. opinionated their claims were. The word cloud will simply tell you what words are in that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3e81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using our list_of_topics_to_visualize list in the customizable settings, \n",
    "# produce a graph for each topic\n",
    "for topic_id in list_of_topics_to_visualize:\n",
    "    if topic_id <= len(lda_model.print_topics()) - 1:\n",
    "        visualize_single_topic_subjectivity_vs_sentiment(df, topic_id)\n",
    "        visualize_topic_word_cloud(lda_model, topic_id)\n",
    "    else:\n",
    "        print(\"Error, topic_id {0} not availible\".format(topic_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f184ac6b",
   "metadata": {},
   "source": [
    "## Generating 2D and 3D Cluster Graphs of Topics (K means Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b465b0",
   "metadata": {},
   "source": [
    "Generate a visual of number of topic clusters vs inertia (WCSS score) of the model (low inertia is good). We want to find the optimal number of topic clusters to use, and you can use the \"elbow method\" on this graph to find it. Ideally, the number of clusters is as small as possible and the inertia is also as small as possible. So to find the point where increasing the number of clusters gives diminishing returns on the inertia score, we imagine the line graph as an arm and use the number of clusters at the \"elbow\", or the point on the graph where the slope becomes much closer to 0 than the point before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65098654",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_optimal_cluster_count(df_topics= df_topics,\n",
    "                                max_clusters= kmeans_settings[\"max_clusters\"],\n",
    "                                pca_components= kmeans_settings[\"pca_components\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cabb2b",
   "metadata": {},
   "source": [
    "This will generate the topic space in 2D with every article represented as a single point. Very similar to the t-SNE clustering graph, this visualization has the same goal: show topic clusters of articles that are very similar in topic and overall theme. Points closer together means they have very similar topics, while points further apart will be less related. This visualization uses the relevance of each topic to an article as the data behind the clustering. Articles are colored by the user-selected number of clusters (not by main topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bab1e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_kmeans_clustering(df_topics= df_topics,\n",
    "                            num_clusters = kmeans_settings[\"num_clusters\"],\n",
    "                            pca_components = kmeans_settings[\"pca_components\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe2adb",
   "metadata": {},
   "source": [
    "This visualization is a 3D version of the one above, with a 3rd dimension added to represent sentiment value of each article. So the 2D xy-plane will be exactly the same as the visualization above--meant to represent articles in the topic space and how they are related by topic. The third dimension will show every article's sentiment value, to show how clusters or individual articles differ by sentiment (or are closely related!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b5c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_kmeans_clustering_3d(dataframe = df,\n",
    "                                      df_topics= df_topics,\n",
    "                                      num_clusters = kmeans_settings[\"num_clusters\"],\n",
    "                                      pca_components = kmeans_settings[\"pca_components\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad512fee",
   "metadata": {},
   "source": [
    "## Relevance X Sentiment of Articles K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27d2dd",
   "metadata": {},
   "source": [
    "This visualization is a 2D clustering (using k-means) of the sentiment-weighted topic relevance values of every article. By that, we mean we have the relevance values of each article for every topic. We multiply each of those relevance values by that topic's associated sentiment value for an article, giving us a weighted relevance value based on the sentiment of that article towards that topic.\n",
    "\n",
    "With that information, we use the same clustering algorithm as the 2D k-means graph above to produce a 2D topic space for every article, but this time it is weighted by the article's sentiment towards the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf536faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather weighted sentiment values into a dataframe\n",
    "result = multiply_tuples_in_dataframe(per_topic_df)\n",
    "\n",
    "visualize_kmeans_clustering(df_topics= result,\n",
    "                            num_clusters = kmeans_settings[\"num_clusters\"],\n",
    "                            pca_components = kmeans_settings[\"pca_components\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733cc30",
   "metadata": {},
   "source": [
    "# Sentiment VS Relevance Per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bab4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(lda_model.print_topics())):\n",
    "    plot_sentiment_relevance(per_topic_df[:][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0d17a",
   "metadata": {},
   "source": [
    "# Topic Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4f3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_space = dimension_reduction(df_topics)\n",
    "plot_pca(topic_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467e9fb",
   "metadata": {},
   "source": [
    "# Document Sentiment in Topic Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77283c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_documents(df_topics, topic_space, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d6974",
   "metadata": {},
   "source": [
    "# Document Density in Topic Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbae2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_heatmap(df_topics, topic_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383c7b5",
   "metadata": {},
   "source": [
    "# Topic per Document in Topic Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_documents(per_topic_df, topic_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9e666",
   "metadata": {},
   "source": [
    "# Sentiment Sum Heatmap in Topic Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_heatmap(df_topics, topic_space, df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "434hw",
   "language": "python",
   "name": "434hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
