{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54d0275",
   "metadata": {},
   "source": [
    "# Pre Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4fd4c",
   "metadata": {},
   "source": [
    "These functions are responsible for generating the objects needed for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c738657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_per_topic_tuple_df(topic_level_sentiment, df_topics):\n",
    "    \"\"\"\n",
    "    This function takes in two previously generated objects; a dictionary of \n",
    "    articles that contain the sentiment analysis value for each topic, and \n",
    "    a dataframe of articles (rows) and topics (columns) where each cell is the \n",
    "    relevancy of the topic for that article.\n",
    "    \n",
    "    It combines the two objects into one dataframe of articles (rows) and topics \n",
    "    (columns) where each cell is a tuple (sentiment, relevancy) of that article for \n",
    "    that topic. Returns the tuple dataframe.\n",
    "    \"\"\"\n",
    "    # turn to dataframe\n",
    "    topic_level_sentiment_df = dictionary_to_data_frame(topic_level_sentiment)\n",
    "    # transpose rows and columns\n",
    "    topic_level_sentiment_df = topic_level_sentiment_df.transpose()\n",
    "\n",
    "    def to_int(str):\n",
    "        return int(str)\n",
    "    # rename str names to ints\n",
    "    topic_level_sentiment_df =  topic_level_sentiment_df.rename(to_int,\n",
    "                                                                axis= 'columns')\n",
    "\n",
    "    # zip the two dfs together\n",
    "    tuple_df = pd.concat([topic_level_sentiment_df, df_topics]).\\\n",
    "                        groupby(level=0).agg(tuple)\n",
    "    return tuple_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8a4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe_for_datavis(dataframe, lda_model, corpus):\n",
    "    \"\"\"\n",
    "    This function takes our generated sentiment analysis dataframe, and the LDA \n",
    "    model and corpus for the texts. It adds several variables to the dataframe \n",
    "    (Topics, Main Topic, Main Topic Score (relevancy), Shortened Address) and \n",
    "    then sorts them based on main topic for data visualization. Returns a new, \n",
    "    processed dataframe.\n",
    "    \"\"\"\n",
    "    # make deep copy of dataframe to prevent changes to original\n",
    "    df = dataframe.copy(deep = True)\n",
    "\n",
    "    df['Topics'] = lda_model.get_document_topics(corpus)\n",
    "\n",
    "    sf = pd.DataFrame(data=df['Topics'])\n",
    "    af = pd.DataFrame()\n",
    "\n",
    "    df_topic_list = []\n",
    "    df_score_list = []\n",
    "\n",
    "    # here we find most relevant topic for each article\n",
    "    for ind in sf.index:\n",
    "        # print(sf['Topics'][ind])\n",
    "        rtl = sf['Topics'][ind]\n",
    "        relevant_topic = -1\n",
    "        relevant_topic_score = 0\n",
    "        # print(\"rtl:\" , rtl)\n",
    "        for (topic,score) in rtl:\n",
    "            # print(topic, \" \" , score)\n",
    "            if score > relevant_topic_score:\n",
    "                relevant_topic = topic\n",
    "                relevant_topic_score = score\n",
    "        df_topic_list.append(relevant_topic)\n",
    "        df_score_list.append(relevant_topic_score)\n",
    "\n",
    "    # We add main topic and the main topic's relevancy score\n",
    "    df['Main Topic'] = df_topic_list # add most relevant topic to df\n",
    "    df['Main Topic Score'] = df_score_list\n",
    "\n",
    "    associated_words = []\n",
    "    for topic_id in df['Main Topic']:\n",
    "        associated_words.append([([word for word, _ in lda_model.show_topic(topic_id)])])\n",
    "    df['Associated Words'] = associated_words\n",
    "\n",
    "    # grab list of urls for parsing\n",
    "    url_list = df['URL'].to_list()\n",
    "\n",
    "    # iterate through urls and parse them into their base site names\n",
    "    for i, url in enumerate(url_list):\n",
    "        if 'https://' in url:\n",
    "            url = url.split(\"https://\")[1]\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "        elif 'http://' in url:\n",
    "            url = url.split(\"http://\")[1]\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "        else:\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "\n",
    "    # shorten the urls to make them easier to read in hover text\n",
    "    df['Shortened Address'] = url_list\n",
    "\n",
    "    # sort df by main topic so it is in order in the graph.\n",
    "    df = df.sort_values('Main Topic')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae90b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_relevancy_dataframe(lda_model):\n",
    "    \"\"\"\n",
    "    Taking in our LDA model, creates a new dataframe of all articles (rows) and \n",
    "    all topics (columns) where a cell corresponds to the relevancy of that \n",
    "    topic on that article. So cell [2,3] with a value of 0.988 means article 2 \n",
    "    has a 0.988 relevancy score for Topic 3.\n",
    "    \"\"\"\n",
    "    # gather all topics per document as a list of lists of tuples\n",
    "    document_topics = [lda_model.get_document_topics \\\n",
    "                       (item, minimum_probability = 0.0) for item in corpus]\n",
    "\n",
    "    # get the num of topics to add to df\n",
    "    topic_cols = [x[0] for x in document_topics[0]]\n",
    "\n",
    "    # make df with topics\n",
    "    df_topics = pd.DataFrame(columns = topic_cols)\n",
    "\n",
    "    for i in document_topics:\n",
    "        topic_scores = [x[1] for x in i]\n",
    "        df_topics.loc[len(df_topics.index)] = topic_scores\n",
    "\n",
    "    # document_topics\n",
    "    return df_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45642086",
   "metadata": {},
   "source": [
    "# Data Visualization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ead48",
   "metadata": {},
   "source": [
    "Below contains our functions that generate data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d665a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_articles_on_main_topic(df, plot_type='scatter'):\n",
    "    \"\"\"\n",
    "    Taking in our (pre-processed) main dataframe, generates either a scatter or box plot\n",
    "    of all articles sorted by their main topic and plotted along their sentiment values.\n",
    "    Also returns useful information through hovertext.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the article data.\n",
    "        plot_type (str): Type of plot ('scatter' or 'box'). Default is 'scatter'.\n",
    "    \"\"\"\n",
    "    # Define a common category order for the main topics\n",
    "    common_category_order = df['Main Topic'].unique()\n",
    "\n",
    "    if plot_type == 'scatter':\n",
    "        # Create a scatter plot with x, y, and color from our df.\n",
    "        fig = px.scatter(df, x=\"Main Topic\", y=\"Sentiment Score\",\n",
    "                         size=\"Main Topic Score\",\n",
    "                         custom_data=['Associated Words',\n",
    "                                      'Shortened Address',\n",
    "                                      'Sentiment Label',\n",
    "                                      'Main Topic Score'],\n",
    "                         title=\"Articles Sorted By Main Topic\")\n",
    "        # Set the y-axis range to make it symmetrical around zero\n",
    "        y_max = df['Sentiment Score'].abs().max()\n",
    "        fig.update_yaxes(range=[-y_max, y_max])\n",
    "\n",
    "    elif plot_type == 'box':\n",
    "        # Create a box plot with main topic on the x-axis and sentiment score on the y-axis\n",
    "        fig = px.box(df, x=\"Main Topic\", y=\"Sentiment Score\",\n",
    "                     category_orders={\"Main Topic\": common_category_order},  # Set category order\n",
    "                     custom_data=['Associated Words',\n",
    "                                  'Shortened Address',\n",
    "                                  'Sentiment Label',\n",
    "                                  'Main Topic Score'],\n",
    "                     title=\"Articles Sorted By Main Topic\",\n",
    "                     labels={'Main Topic': 'Main Topic', 'Sentiment Score': 'Sentiment Score'})\n",
    "        # Set the y-axis range to make it symmetrical around zero\n",
    "        y_max = df['Sentiment Score'].abs().max()\n",
    "        fig.update_yaxes(range=[-y_max, y_max])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type. Choose 'scatter' or 'box'.\")\n",
    "\n",
    "    # Set the hover text to show what was in custom_data\n",
    "    fig.update_traces(hovertemplate=\"<br>\".join(\\\n",
    "                                [\"Associated Words: %{customdata[0]}\",\n",
    "                                 \"Address: %{customdata[1]}\",\n",
    "                                 \"Sentiment Label: %{customdata[2]}\",\n",
    "                                 \"Main Topic Score : %{customdata[3]}\"]))\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c891a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topic_cluster_tsne(lda_model, df):\n",
    "    \"\"\"\n",
    "    Taking in both our main (pre-processed) dataframe and our LDA model, \n",
    "    generates a topic clustering graph based on the topic relevancy of \n",
    "    each article's topic. Displays the clustering in a 2D space.\n",
    "    \"\"\"\n",
    "    num_topics = len(lda_model.print_topics())\n",
    "\n",
    "    #Get Topic Weights\n",
    "    topic_weights = []\n",
    "    for i in df[\"Topics\"]:\n",
    "        per_doc_list = [None] * num_topics\n",
    "        #print(len(per_doc_list))\n",
    "        for x in i:\n",
    "            #print(x)\n",
    "            per_doc_list[x[0]] = x[1]\n",
    "        topic_weights.append(per_doc_list)\n",
    "\n",
    "    # Array of topic weights  \n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).to_numpy()\n",
    "\n",
    "    # tSNE Model Creation\n",
    "    tsne_model = TSNE(n_components=2, verbose=1,\n",
    "                  random_state=0, angle=.99, \n",
    "                  init='pca', perplexity = (arr.shape[0] - 1) / 3)\n",
    "\n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "    # Formatting\n",
    "    # Turn them into ints so we can sort by main topic, then back to str\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)\n",
    "\n",
    "    # Sort by main topic to make the legend pretty\n",
    "    df = df.sort_values(by=['Main Topic'],ascending = True)\n",
    "\n",
    "    # This makes it so we can use main topic as categorical data\n",
    "    df['Main Topic'] = df['Main Topic'].apply(str)\n",
    "\n",
    "    # Creating the cluster graph in plotly\n",
    "    fig_cluster = px.scatter(df, x = tsne_lda[:,0],y = tsne_lda[:,1],\n",
    "                        custom_data = ['Associated Words',\n",
    "                                       'Shortened Address',\n",
    "                                       'Sentiment Label',\n",
    "                                       'Main Topic'],\n",
    "                        color = \"Main Topic\",\n",
    "                        size = \"Main Topic Score\",\n",
    "                        title = \"Topic Clustering Graph\",\n",
    "                        labels = dict(color = \"Main Topic\"))\n",
    "\n",
    "    # Set the hover text to show whatwas in custom_data\n",
    "    fig_cluster.update_traces(hovertemplate=\\\n",
    "                              \"<br>\".join([\"Associated Words: %{customdata[0]}\",\n",
    "                                           \"Address: %{customdata[1]}\",\n",
    "                                           \"Sentiment Label: %{customdata[2]}\",\n",
    "                                           \"Main Topic:%{customdata[3]}\"])\n",
    "                          )\n",
    "\n",
    "    fig_cluster.show()\n",
    "\n",
    "    # Turn back to int\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f8d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_topic_subjectivity_vs_sentiment(df, topic_num):\n",
    "    \"\"\"\n",
    "    Taking in the main (pre-processed) dataframe and an int corresponding to \n",
    "    the topic number you want to visualize, generates a 2D plot of all articles \n",
    "    with that topic number as their main topic, plotted along their \n",
    "    sentiment scores and their subjectivity scores (higher is more subjective).\n",
    "    \"\"\"\n",
    "    df_topic = df[df['Main Topic'] == topic_num]\n",
    "\n",
    "\n",
    "    t_string = \"Sentiment Analysis on Topic \" + str(topic_num)\n",
    "    fig = px.scatter(df_topic, x = \"Subjectivity Score\", y = \"Sentiment Score\",\n",
    "                size = \"Main Topic Score\", hover_name = \"Shortened Address\", title = t_string)\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83574bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topic_word_cloud(lda_model, topic_id):\n",
    "    # Initialize the WordCloud generator\n",
    "    cloud = WordCloud(background_color='white',\n",
    "                    width=2500,\n",
    "                    height=1800,\n",
    "                    max_words=10,\n",
    "                    colormap='tab10',  # You can choose a different colormap\n",
    "                    prefer_horizontal=1.0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))  # Create a single plot\n",
    "    topic_words = dict(lda_model.show_topic(topic_id, topn=10))  # Get the top words for the topic\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    ax.imshow(cloud)\n",
    "    ax.set_title('Topic ' + str(topic_id), fontdict=dict(size=16))\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12e40a",
   "metadata": {},
   "source": [
    "# K-means clustering and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe1e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimal_cluster_count(df_topics, max_clusters, pca_components):\n",
    "    \"\"\"\n",
    "    Taking in our topic relevancy dataframe, the max cluster count that you \n",
    "    want to stop at, and the number of PCA components to use, generates a \n",
    "    visualization of the inertia score of our clustering model for every number \n",
    "    of clusters up to the max.\n",
    "    \"\"\"\n",
    "    wcss = []\n",
    "\n",
    "    # Set to number of components\n",
    "    pca = PCA(pca_components)\n",
    "    # Apply principled component analysis\n",
    "    data = pca.fit_transform(df_topics)\n",
    "\n",
    "    for i in range(2, max_clusters):\n",
    "        model = KMeans(n_clusters = i, init = \"k-means++\", n_init = 10)\n",
    "        model.fit(data)\n",
    "        wcss.append(model.inertia_)\n",
    "\n",
    "    # Plot inertia for the different number of clusters\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(2, max_clusters), wcss)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d56249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kmeans_clustering(df_topics, num_clusters, pca_components):\n",
    "    \"\"\"\n",
    "    Taking in our topic relevancy dataframe, the number of clusters, and the \n",
    "    number of PCA components, generates a k-means clustering using those \n",
    "    parameters and visualizes it in a 2D space. The clustering is based solely \n",
    "    on the topic relevancy for each article. Also generates centers of each \n",
    "    cluster shown with an 'X'.\n",
    "    \"\"\"\n",
    "    # Set to number of components\n",
    "    pca = PCA(pca_components)\n",
    "    # Apply principled component analysis\n",
    "    data = pca.fit_transform(df_topics)\n",
    "\n",
    "    # Creating KMeans model\n",
    "    model = KMeans(n_clusters = num_clusters, init = \"k-means++\", n_init = 10)\n",
    "    label = model.fit_predict(data)\n",
    "    centers = np.array(model.cluster_centers_)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    uniq = np.unique(label)\n",
    "\n",
    "    # Creating the scatter plot of all articles\n",
    "    for i in uniq:\n",
    "        plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)\n",
    "    # This is done to find the centroid for each clusters.\n",
    "    plt.scatter(centers[:,0], centers[:,1], marker=\"x\", color='k')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9aaa7",
   "metadata": {},
   "source": [
    "# Sentiment X Topic Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68588ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_tuples_in_dataframe(input_df):\n",
    "    \"\"\"\n",
    "    Multiply the elements of tuples in the DataFrame to create a new DataFrame of float values.\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): The input DataFrame containing tuples.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with float values resulting from multiplying the tuples.\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with the same structure as the input DataFrame\n",
    "    result_df = pd.DataFrame(index=input_df.index, columns=input_df.columns)\n",
    "\n",
    "    for column in input_df.columns:\n",
    "        # Use apply to multiply the elements of each tuple and store the result in the new DataFrame\n",
    "        result_df[column] = input_df[column].apply(lambda x: x[0] * x[1])\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_relevance(data):\n",
    "    # Separate the sentiment and relevance values into separate lists\n",
    "    sentiments, relevances = zip(*data)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(sentiments, relevances, c='blue', marker='o', alpha=0.5)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title('Sentiment vs Relevance')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Relevance')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(doc_topic_matrix):\n",
    "    n_components = 2  # Number of dimensions for topic space\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    topic_space = pca.fit_transform(doc_topic_matrix.T)\n",
    "    return topic_space\n",
    "\n",
    "def plot_pca(topic_space):\n",
    "    topic_labels = [str(i) for i in range(len(topic_space))]\n",
    "\n",
    "    # Create a scatterplot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(*zip(*topic_space), marker='o', s=100, c='b')\n",
    "\n",
    "    # Add labels for each point\n",
    "    \n",
    "    for i, (x, y) in enumerate(topic_space):\n",
    "        plt.annotate(topic_labels[i], (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "\n",
    "    plt.title(\"Intertopic Distance Map\")\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7787663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_documents(doc_topic_matrix, topic_space, df_main): \n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    colormap = plt.get_cmap('coolwarm')\n",
    "    colors = []\n",
    "\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    document_space = []\n",
    "    for doc_id in range(n_docs):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        for topic_id in range(n_topics):\n",
    "            # Multiply the coordinates of the topic by the relavence of the topic\n",
    "            x += topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id] \n",
    "            y += topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "        document_space.append((x,y))\n",
    "    \n",
    "    for sentiment in df_main[\"Sentiment Score\"]:\n",
    "        # Normalize sentiment score to the range [0, 1]\n",
    "        normalized_sentiment = (sentiment + 0.2) / 0.4  # Map -0.2 to 0.2 to [0, 1]\n",
    "        sentiment_color = colormap(normalized_sentiment, alpha=1.0)\n",
    "        colors.append(sentiment_color)\n",
    "\n",
    "\n",
    "\n",
    "    x_coords, y_coords = zip(*document_space)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x_coords, y_coords, marker='o', label='Documents', c=colors)\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Scatter Plot of Data Points')\n",
    "\n",
    "    # Plot topics\n",
    "    x_coords_topics = [x for x, _ in topic_space]\n",
    "    y_coords_topics = [y for _, y in topic_space]\n",
    "    plt.scatter(x_coords_topics, y_coords_topics, color='black', marker='o', s=5, label='Topics')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_documents(df, topic_space): # Add entire article sentiment to this...\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    colormap = plt.get_cmap('coolwarm')\n",
    "    colors = []\n",
    "\n",
    "    # Create separate DataFrames for the first and second values\n",
    "    doc_topic_matrix = pd.DataFrame()\n",
    "    sentiment_matrix = pd.DataFrame()\n",
    "\n",
    "    for column in df.columns:\n",
    "        doc_topic_matrix[column] = df[column].apply(lambda x: x[0])\n",
    "        sentiment_matrix[column] = df[column].apply(lambda x: x[1])\n",
    "\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    document_topic_space = []\n",
    "    for doc_id in range(n_docs):\n",
    "        for topic_id in range(n_topics):\n",
    "            if doc_topic_matrix[topic_id][doc_id] > 0.1:\n",
    "                # Multiply the coordinates of the topic by the relavence of the topic\n",
    "                x = topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id] \n",
    "                y = topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "                document_topic_space.append((x,y))\n",
    "                # Get color of point based on the sentiment\n",
    "                sentiment_color = colormap(sentiment_matrix[topic_id][doc_id])\n",
    "                colors.append(sentiment_color)\n",
    "\n",
    "\n",
    "    x_coords, y_coords = zip(*document_topic_space)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x_coords, y_coords, marker='o', label='Data Points', c=colors)\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Scatter Plot of Data Points')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44177443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_heatmap(doc_topic_matrix, topic_space):\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "\n",
    "    for doc_id in range(n_docs):\n",
    "        x = 0\n",
    "        y = 0\n",
    "\n",
    "        for topic_id in range(n_topics):\n",
    "            # Multiply the coordinates of the topic by the relevance of the topic\n",
    "            x += topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id]\n",
    "            y += topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "\n",
    "        x_coords.append(x)\n",
    "        y_coords.append(y)\n",
    "\n",
    "    # Create a 2D grid for the heatmap\n",
    "    x_min, x_max = -max(x_coords), max(x_coords)\n",
    "    y_min, y_max = -max(y_coords), max(y_coords)\n",
    "\n",
    "    resolution = 2\n",
    "    x_range = int((x_max - x_min) * resolution) + 1\n",
    "    y_range = int((y_max - y_min) * resolution) + 1\n",
    "\n",
    "    heatmap = [[0 for _ in range(x_range + 1)] for _ in range(y_range + 1)]\n",
    "\n",
    "    for x, y in zip(x_coords, y_coords):\n",
    "        x_idx = int((x * resolution) + x_range/2)\n",
    "        y_idx = int((y * resolution) + y_range/2)\n",
    "        heatmap[y_idx][x_idx] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(heatmap, cmap='coolwarm', extent=[x_min, x_max, y_min, y_max], origin='lower')\n",
    "    plt.colorbar(label='Topic_Density')\n",
    "\n",
    "    # Plot topics\n",
    "    x_coords_topics = [x for x, _ in topic_space]\n",
    "    y_coords_topics = [y for _, y in topic_space]\n",
    "    plt.scatter(x_coords_topics, y_coords_topics, color='black', marker='o', s=10, label='Data Points')\n",
    "\n",
    "\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Topic Heatmap')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eefc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_heatmap(doc_topic_matrix, topic_space, df_main):\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    sentiment_scores = df_main[\"Sentiment Score\"]\n",
    "\n",
    "    for doc_id in range(n_docs):\n",
    "        x = 0\n",
    "        y = 0\n",
    "\n",
    "        for topic_id in range(n_topics):\n",
    "            # Multiply the coordinates of the topic by the relevance of the topic\n",
    "            x += topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id]\n",
    "            y += topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "\n",
    "        x_coords.append(x)\n",
    "        y_coords.append(y)\n",
    "\n",
    "    # Create a 2D grid for the heatmap\n",
    "    x_min, x_max = -max(x_coords), max(x_coords)\n",
    "    y_min, y_max = -max(y_coords), max(y_coords)\n",
    "\n",
    "    resolution = 2\n",
    "    x_range = int((x_max - x_min) * resolution) + 1\n",
    "    y_range = int((y_max - y_min) * resolution) + 1\n",
    "\n",
    "    heatmap = [[0 for _ in range(x_range + 1)] for _ in range(y_range + 1)]\n",
    "    heatmap_normalization_factor = [[0 for _ in range(x_range + 1)] for _ in range(y_range + 1)]\n",
    "\n",
    "    for x, y, sentiment in zip(x_coords, y_coords, sentiment_scores):\n",
    "        x_idx = int((x * resolution) + x_range/2)\n",
    "        y_idx = int((y * resolution) + y_range/2)\n",
    "        heatmap[y_idx][x_idx] += sentiment\n",
    "        heatmap_normalization_factor[y_idx][x_idx] += 1\n",
    "    \n",
    "    # Normalize the values to get an average sentiment\n",
    "    for y_idx in range(len(heatmap)):\n",
    "        for x_idx in range(len(heatmap[y_idx])):\n",
    "            if heatmap_normalization_factor[y_idx][x_idx] != 0:\n",
    "                heatmap[y_idx][x_idx] /= heatmap_normalization_factor[y_idx][x_idx]\n",
    "            \n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(heatmap, cmap='coolwarm', extent=[x_min, x_max, y_min, y_max], origin='lower')\n",
    "    plt.colorbar(label='Sentiment Score')\n",
    "\n",
    "    # Plot topics\n",
    "    x_coords_topics = [x for x, _ in topic_space]\n",
    "    y_coords_topics = [y for _, y in topic_space]\n",
    "    plt.scatter(x_coords_topics, y_coords_topics, color='black', marker='o', s=10, label='Data Points')\n",
    "\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Sentiment Heatmap')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "434hw",
   "language": "python",
   "name": "434hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
