{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54d0275",
   "metadata": {},
   "source": [
    "# Pre Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4fd4c",
   "metadata": {},
   "source": [
    "These functions are responsible for generating the objects needed for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c738657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes in two previously generated objects; a dictionary of articles that contain the sentiment analysis\n",
    "value for each topic, and a dataframe of articles (rows) and topics (columns) where each cell is the relevancy of the\n",
    "topic for that article.\n",
    "\n",
    "It combines the two objects into one dataframe of articles (rows) and topics (columns) where each cell is a tuple\n",
    "(sentiment, relevancy) of that article for that topic. Returns the tuple dataframe.\n",
    "\"\"\"\n",
    "def create_per_topic_tuple_df(topic_level_sentiment, df_topics):\n",
    "    topic_level_sentiment_df = dictionaryToDataFrame(topic_level_sentiment)#turn to dataframe\n",
    "    topic_level_sentiment_df = topic_level_sentiment_df.transpose() #transpose rows and columns\n",
    "    \n",
    "    def to_int(str):\n",
    "        return int(str)\n",
    "    topic_level_sentiment_df = topic_level_sentiment_df.rename(to_int, axis= 'columns')#rename str names to ints\n",
    "    \n",
    "    \n",
    "    tuple_df = pd.concat([topic_level_sentiment_df, df_topics]).groupby(level=0).agg(tuple)#zip the two dfs together\n",
    "    return tuple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8a4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function takes our generated sentiment analysis dataframe, and the LDA model and corpus for the texts. It adds\n",
    "several variables to the dataframe (Topics, Main Topic, Main Topic Score (relevancy), Shortened Address) and \n",
    "then sorts them based on main topic for data visualization. Returns a new, processed dataframe.\n",
    "\"\"\"\n",
    "def preprocess_dataframe_for_datavis(dataframe, lda_model, corpus):\n",
    "    \n",
    "    #make deep copy of dataframe to prevent changes to original\n",
    "    df = dataframe.copy(deep = True)\n",
    "    \n",
    "    df['Topics'] = lda_model.get_document_topics(corpus)\n",
    "\n",
    "\n",
    "    sf = pd.DataFrame(data=df['Topics'])\n",
    "    af = pd.DataFrame()\n",
    "\n",
    "    df_topic_list = []\n",
    "    df_score_list = []\n",
    "\n",
    "    for ind in sf.index: #here we find most relevant topic for each article\n",
    "        #print(sf['Topics'][ind])\n",
    "        rtl = sf['Topics'][ind]\n",
    "        relevant_topic = -1\n",
    "        relevant_topic_score = 0\n",
    "        #print(\"rtl:\" , rtl)\n",
    "        for (topic,score) in rtl:\n",
    "            #print(topic, \" \" , score)\n",
    "            if score > relevant_topic_score:\n",
    "                relevant_topic = topic\n",
    "                relevant_topic_score = score\n",
    "        df_topic_list.append(relevant_topic)\n",
    "        df_score_list.append(relevant_topic_score)\n",
    "\n",
    "    #We add main topic and the main topic's relevancy score\n",
    "    df['Main Topic'] = df_topic_list #add most relevant topic to df\n",
    "    df['Main Topic Score'] = df_score_list\n",
    "    \n",
    "    #grab list of urls for parsing\n",
    "    url_list = df['URL'].to_list()\n",
    "\n",
    "    #iterate through urls and parse them into their base site names\n",
    "    for i, url in enumerate(url_list):\n",
    "        if 'https://' in url:\n",
    "            url = url.split(\"https://\")[1]\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "        else:\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    df['Shortened Address'] = url_list #shorten the urls to make them easier to read in hover text\n",
    "\n",
    "    df = df.sort_values('Main Topic') #sort df by main topic so it is in order in the graph.\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae90b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking in our LDA model, creates a new dataframe of all articles (rows) and all topics (columns) where a cell corresponds\n",
    "to the relevancy of that topic on that article. So cell [2,3] with a value of 0.988 means article 2 has a 0.988 \n",
    "relevancy score for Topic 3.\n",
    "\"\"\"\n",
    "def generate_topic_relevancy_dataframe(LDA_model):\n",
    "    #gather all topics per document as a list of lists of tuples\n",
    "    document_topics = [LDA_model.get_document_topics(item, minimum_probability = 0.0) for item in corpus]\n",
    "\n",
    "    topic_cols = [x[0] for x in document_topics[0]] #get the num of topics to add to df\n",
    "\n",
    "\n",
    "    df_topics = pd.DataFrame(columns = topic_cols)#make df with topics\n",
    "    \n",
    "    \n",
    "    for i in document_topics:\n",
    "        topic_scores = [x[1] for x in i]\n",
    "        df_topics.loc[len(df_topics.index)] = topic_scores\n",
    "    \n",
    "    #this gathers sentiment score, subjectivity score, and main topic of all articles\n",
    "    #df_slice = df_main_topic.iloc[:,[1,3,8]]\n",
    "\n",
    "    #document_topics\n",
    "    return df_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45642086",
   "metadata": {},
   "source": [
    "# Data Visualization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ead48",
   "metadata": {},
   "source": [
    "Below contains our functions that generate data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking in our (pre-processed) main dataframe, generates a plot of all articles sorted by their main topic, and plotted\n",
    "along their sentiment values. Also returns useful information through hovertext.\n",
    "\"\"\"\n",
    "def visualize_all_articles_on_main_topic(df):\n",
    "    \n",
    "    fig_w_topics = px.scatter(df, x=\"Main Topic\", y=\"Sentiment Score\",\n",
    "                         size=\"Main Topic Score\", custom_data = ['Shortened Address', 'Sentiment Label', 'Main Topic Score'],\n",
    "                         title=\"Articles Sorted By Main Topic\")#creating a scatter plot with x, y, and color from our df. \n",
    "    #The custom_data is what we will add to our hover text\n",
    "\n",
    "    fig_w_topics.update_traces(hovertemplate=\"<br>\".join([\"Address: %{customdata[0]}\",\n",
    "                                                     \"Sentiment Label: %{customdata[1]}\",\n",
    "                                                     \"Main Topic Score :%{customdata[2]}\"])\n",
    "                          )#set the hover text to show whatwas in custom_data\n",
    "\n",
    "    fig_w_topics.update_xaxes(type=\"category\")#turn into categorical not continuous data for the x-axis\n",
    "\n",
    "    fig_w_topics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking in both our main (pre-processed) dataframe and our LDA model, generates a topic clustering graph based on the\n",
    "topic relevancy of each article's topic. Displays the clustering in a 2D space.\n",
    "\"\"\"\n",
    "def visualize_topic_cluster_TSNE(LDA_model, df):\n",
    "    num_topics = len(LDA_model.print_topics())\n",
    "    #print(num_topics)\n",
    "\n",
    "    #Get Topic Weights\n",
    "    topic_weights = []\n",
    "    for i in df[\"Topics\"]:\n",
    "        per_doc_list = [None] * num_topics\n",
    "        #print(len(per_doc_list))\n",
    "        for x in i:\n",
    "            #print(x)\n",
    "            per_doc_list[x[0]] = x[1]\n",
    "        topic_weights.append(per_doc_list)\n",
    "\n",
    "    # Array of topic weights    \n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).to_numpy()\n",
    "\n",
    "    # Dominant topic number in each doc\n",
    "    topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "    #print(arr)\n",
    "\n",
    "    # tSNE Model Creation\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, \n",
    "                  random_state=0, angle=.99, \n",
    "                  init='pca', perplexity = (arr.shape[0] - 1) / 3)\n",
    "    \n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "    #Color palette for the clusters\n",
    "    mycolors = np.array([color for name, color in mcolors.CSS4_COLORS.items()])\n",
    "\n",
    "    #Formatting\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)#turn them into ints so we can sort by main topic, then back to str\n",
    "\n",
    "    df = df.sort_values(by=['Main Topic'],ascending = True)#sort by main topic to make the legend pretty\n",
    "\n",
    "    df['Main Topic'] = df['Main Topic'].apply(str) #this makes it so we can use main topic as categorical data\n",
    "\n",
    "    #creating the cluster graph in plotly\n",
    "    fig_cluster = px.scatter(df, x = tsne_lda[:,0],y = tsne_lda[:,1],\n",
    "                        custom_data = ['Shortened Address', 'Sentiment Label', 'Main Topic'],\n",
    "                        color = \"Main Topic\",#mycolors[topic_num],\n",
    "                        size = \"Main Topic Score\",\n",
    "                        title = \"Topic Clustering Graph\",\n",
    "                        labels = dict(color = \"Main Topic\"))\n",
    "\n",
    "    fig_cluster.update_traces(hovertemplate=\"<br>\".join([\"Address: %{customdata[0]}\",\n",
    "                                                     \"Sentiment Label: %{customdata[1]}\",\n",
    "                                                     \"Main Topic:%{customdata[2]}\"])\n",
    "                          )#set the hover text to show whatwas in custom_data\n",
    "    \n",
    "    fig_cluster.show()\n",
    "\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)#turn back to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking in the main (pre-processed) dataframe and an int corresponding to the topic number you want to visualize, \n",
    "generates a 2D plot of all articles with that topic number as their main topic, plotted along their sentiment scores\n",
    "and their subjectivity scores (higher is more subjective).\n",
    "\"\"\"\n",
    "def visualize_single_topic_subjectivity_vs_sentiment(df, topic_num):\n",
    "    df_topic = df[df['Main Topic'] == topic_num]\n",
    "\n",
    "\n",
    "    t_string = \"Sentiment Analysis on Topic \" + str(topic_num)\n",
    "    fig = px.scatter(df_topic, x = \"Subjectivity Score\", y = \"Sentiment Score\", \n",
    "                size = \"Main Topic Score\", hover_name = \"Shortened Address\", title = t_string)\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12e40a",
   "metadata": {},
   "source": [
    "# K-means clustering and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking in our topic relevancy dataframe, the max cluster count that you want to stop at, and the number of PCA \n",
    "components to use, generates a visualization of the inertia score of our clustering model for every number of \n",
    "clusters up to the max.\n",
    "\"\"\"\n",
    "def visualize_optimal_cluster_count(df_topics, max_clusters, pca_components):\n",
    "    wcss = []\n",
    "    \n",
    "    pca = PCA(pca_components) #set to number of components\n",
    "    data = pca.fit_transform(df_topics) #apply principled component analysis\n",
    "   \n",
    "    for i in range(2, max_clusters):\n",
    "       model = KMeans(n_clusters = i, init = \"k-means++\", n_init = 10)\n",
    "       model.fit(data)\n",
    "       wcss.append(model.inertia_)\n",
    "    \n",
    "    #plot inertia for the different number of clusters\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(2, max_clusters), wcss)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking in our topic relevancy dataframe, the number of clusters, and the number of PCA components, generates a \n",
    "k-means clustering using those parameters and visualizes it in a 2D space. The clustering is based solely on the topic\n",
    "relevancy for each article. Also generates centers of each cluster shown with an 'X'.\n",
    "\"\"\"\n",
    "def visualize_kmeans_clustering(df_topics, num_clusters, pca_components):\n",
    "    pca = PCA(pca_components) #set to number of components\n",
    "    data = pca.fit_transform(df_topics) #apply principled component analysis\n",
    "    \n",
    "    #creating KMeans model\n",
    "    model = KMeans(n_clusters = num_clusters, init = \"k-means++\", n_init = 10)\n",
    "    label = model.fit_predict(data)\n",
    "    centers = np.array(model.cluster_centers_)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    uniq = np.unique(label)\n",
    "    \n",
    "    #creating the scatter plot of all articles\n",
    "    for i in uniq:\n",
    "       plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)\n",
    "    plt.scatter(centers[:,0], centers[:,1], marker=\"x\", color='k')#This is done to find the centroid for each clusters.\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
