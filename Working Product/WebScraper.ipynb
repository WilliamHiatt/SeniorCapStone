{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd185ac0",
   "metadata": {},
   "source": [
    "# Webscraping Functions\n",
    "All the webscraping takes place in this code block. Webscraping is being done by Newspaper3k. Webscrapping errors are generated when the article does not contain enough words to avoid the wrong text such as ads and article previews. Also errors on articles that contain repeated phrases (in the case that the article blocks webscraping and we get repeated error messages), and any known social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351de990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_repeated_phrase_count(text):\n",
    "    \"\"\"\n",
    "    Taking in a webscraped text, finds all of the phrases in an article and makes a count of the repeated phrases. Returns\n",
    "    the count of the most repeated phrase in an article.\n",
    "    \"\"\"\n",
    "    # Split the text into phrases (e.g., sentences)\n",
    "    phrases = re.split(r'\\.', text)\n",
    "\n",
    "    # Remove leading and trailing spaces from each phrase\n",
    "    phrases = [phrase.strip() for phrase in phrases if phrase.strip()]\n",
    "\n",
    "    # Count the occurrences of each phrase using Counter\n",
    "    phrase_counts = Counter(phrases)\n",
    "\n",
    "    if not phrase_counts:\n",
    "        return 0\n",
    "\n",
    "    # Find the most common phrase and its count\n",
    "    most_common_phrase, count = phrase_counts.most_common(1)[0]\n",
    "    \n",
    "    return count\n",
    "\n",
    "\n",
    "def filter_scrape_data(text, url):\n",
    "    \"\"\"\n",
    "    Taking in an article text, this performs word counting and repeated phrase counting on the text. If either word count\n",
    "    or repeated phrases do not fit within our filter settings, it returns False and prints some error statements.\n",
    "\n",
    "    If both of our filters pass, we return True to show that it passed this filter.\n",
    "    \"\"\"\n",
    "    strLength = np.char.count(text, ' ') + 1\n",
    "    if strLength < word_count_filter or most_repeated_phrase_count(text) >= repeated_phrase_filter:\n",
    "        print(\"Webscraping failed: Word Count or Repeated Phrase\")\n",
    "        print(url)\n",
    "        print(\"Word count: \" , strLength)\n",
    "        print(\"Repeated phrases: \", most_repeated_phrase_count(text))\n",
    "        print(\"\\n\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_social(url):\n",
    "    \"\"\"\n",
    "    Taking in a single url, checks if the url is from a known social media website using our social_starts_with social media\n",
    "    url list (found in main pipeline settings). If the URL is from any of the social media websites, return False to indicate\n",
    "    the webscraping failed and prints some error statements.\n",
    "\n",
    "    If the URL is not found in this list, returns True to indicate it passed this filter.\n",
    "    \"\"\"\n",
    "    for y in social_starts_with:\n",
    "        if(url.startswith(y)):\n",
    "            print(\"Webscraping failed: Social Media\")\n",
    "            print(url)\n",
    "            print(\"\\n\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def scrapeData(url):\n",
    "    \"\"\"\n",
    "    Our main data scraping function. Taking in an unprocessed URL, performs several filters on it to make sure it is\n",
    "    properly webscraped and the text is something we want in our model.\n",
    "\n",
    "    Filters out social media websites using filter_social(), then gathers the text and filters out the article if it\n",
    "    doesn't follow our word count and repeated phrase settings. \n",
    "\n",
    "    Finally, if it passes all filters, the text is processed and returned successfully.\n",
    "\n",
    "    If at any point the text scraping or other functions fail, returns a \"Couldn't Parse\" error indicating there was a failure\n",
    "    to properly gather the data. All webscraping fails have a unique PARERROR that they correspond to so you can know why\n",
    "    that URL failed to be webscraped.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not filter_social(url):\n",
    "            return \"PARERROR: SocialError\"\n",
    "        else:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            \n",
    "            page_text = (article.text).lower()\n",
    "            \n",
    "            if not filter_scrape_data(page_text, url):\n",
    "                return \"PARERROR: WebBlockerError\"\n",
    "        \n",
    "            page_text = page_text.strip().replace(\"  \",\"\")\n",
    "            page_text = \"\".join([s for s in page_text.splitlines(True) if s.strip(\"\\r\\n\")])\n",
    "            \n",
    "    except:\n",
    "        print(\"Webscraping Error: Couldn't Parse\")\n",
    "        print(url)\n",
    "        print(\"\\n\")\n",
    "        page_text = \"PARERROR: ErrorCouldntParse\"\n",
    "    return page_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
