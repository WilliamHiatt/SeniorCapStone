{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0139d3de",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "This file is used for performing sentiment analysis, both on an article as a whole, and on sentences that contain the main topic(s) related to that article for topic-based sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248255e",
   "metadata": {},
   "source": [
    "# Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5cd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_sentiment_analysis(urls):\n",
    "    \"\"\"\n",
    "    This is the main sentiment analysis pipeline function that is responsible for first creating our sentiment analysis\n",
    "    dataframe. Taking in a list of URLs, it iterates through each of them, webscrapes the article text from the URL,\n",
    "    and then performs sentiment analysis on that text. As it iterates, the data is being added to our sentimentDic dictionary.\n",
    "    After we scrape and analyze all URLs in the list, we turn it into a dataframe, remove all the rows that failed \n",
    "    webscraping, and return the dataframe.\n",
    "    \"\"\"\n",
    "    # Loops through our URLS and scraps the data\n",
    "    # Put all empty dictionaries here\n",
    "    sentimentDic = {}\n",
    "\n",
    "    for count, x in enumerate(urls):\n",
    "        if(count % 10 == 0): #layman's way of showing progress\n",
    "            print(str(count), \" Articles Completed\")\n",
    "\n",
    "        url = x #the url of the article we want to webscrape and analyze\n",
    "\n",
    "        # Send the URL to get scraped, returning the text of the article\n",
    "        page_text = scrapeData(x)\n",
    "\n",
    "        # Runs sentiment analysis. Will need to make a new function and a new dictionary\n",
    "        # for each type of analysis we want to run. Will pass in the page_text, the dic, and\n",
    "        # x (the url)\n",
    "        sentimentDic = sentimentAnalysis(page_text, sentimentDic, url)\n",
    "\n",
    "\n",
    "    # For each analysis we run we need to then convert that dictionary with the following method\n",
    "    df = dictionaryToDataFrame(sentimentDic)\n",
    "    \n",
    "    #Clean dataframe by dropping all rows that failed webscraping\n",
    "    df = drop_failed_webscraping_rows(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab46cb1",
   "metadata": {},
   "source": [
    "# Article Level Sentiment Analysis\n",
    "This function is used to perform sentiment analysis on a single document text. Analysis is done with TextBlob with the help of Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(text, dictionary, url):\n",
    "    \"\"\"\n",
    "    The main function for sentiment analysis of a single article. Takes in the scraped article text, the sentiment analysis\n",
    "    dictionary to add the data to, and the url of the article. We add the url to our dictionary, then perform sentiment\n",
    "    analysis on the text by gathering the polarity (sentiment) and subjectivity scores of the text. We label the sentiment\n",
    "    score Positive-Neutral-Negative and in-between, and then add all of the necessary data we generated to the dictionary.\n",
    "    We then create lists of positive words and negative words, adding those to the dictionary as well. At the bottom, if\n",
    "    the web scraping failed for this article, we add results to the dictionary that signal there was an error. Finally\n",
    "    we return the dictionary with its new entry.\n",
    "    \"\"\"\n",
    "    if(len(dictionary) == 0):\n",
    "        dictionary = {\n",
    "            \"URL\": [],\n",
    "            \"Sentiment Score\": [],\n",
    "            \"Sentiment Label\": [],\n",
    "            \"Subjectivity Score\": [],\n",
    "            \"Positive Words\": [],\n",
    "            \"Negative Words\": [],\n",
    "            \"Text\": []\n",
    "            }\n",
    "        \n",
    "    # If there was an error while parsing the document we will not do any sentiment analysis\n",
    "    # on the article text.\n",
    "    if(text[0:8] != \"PARERROR\"):\n",
    "        # Start the sentiment analysis now\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Get's sentiment and subjectivity\n",
    "        sentiment = doc._.blob.polarity\n",
    "        sentiment = round(sentiment,2)\n",
    "        subjectivity = doc._.blob.subjectivity\n",
    "        subjectivity = round(subjectivity,2)\n",
    "\n",
    "        # Gives positive or negative label\n",
    "        if sentiment >= 0.033 and sentiment <= 0.043:\n",
    "            sent_label = \"Neutral\"\n",
    "        elif sentiment > 0.043 and sentiment < 0.143:\n",
    "            sent_label = \"Neutral Positive\"\n",
    "        elif sentiment > 0.143:\n",
    "            sent_label = \"Positive\"\n",
    "        elif sentiment < 0.033 and sentiment > -0.062:\n",
    "            sent_label = \"Neutral Negative\"\n",
    "        elif sentiment < -0.062:\n",
    "            sent_label = \"Negative\"\n",
    "    \n",
    "        # Appending labels to the dictionary\n",
    "        dictionary[\"Sentiment Label\"].append(sent_label)\n",
    "        dictionary[\"Sentiment Score\"].append(sentiment)\n",
    "        dictionary[\"Subjectivity Score\"].append(subjectivity)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "    \n",
    "        # Creating a list of positive and negative words\n",
    "        for x in doc._.blob.sentiment_assessments.assessments:\n",
    "          if x[1] > 0:\n",
    "            positive_words.append(x[0][0])\n",
    "          elif x[1] < 0:\n",
    "            negative_words.append(x[0][0])\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "    \n",
    "    # Hits here if there was a scraping error\n",
    "    else:\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        dictionary[\"Sentiment Label\"].append(text)\n",
    "        dictionary[\"Sentiment Score\"].append(0.0)\n",
    "        dictionary[\"Subjectivity Score\"].append(0.0)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3cf0fc",
   "metadata": {},
   "source": [
    "# Topic Level Sentiment\n",
    "This code block is used to preform the sentiment analysis based on the topic word(s) of an article. It will perform sentiment on the sentences that contain the topic(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dictionary of all topics, with all their associated topic words in the form {Topic_num: [words]}\n",
    "def create_topic_words_dict(ldamodel):\n",
    "    \"\"\"\n",
    "    Taking in the LDA model, this returns a dictionary of topics, where the values for each topic is a list of its\n",
    "    top 10 associated words.\n",
    "    \"\"\"\n",
    "    my_dict = {i: [token for token, score in ldamodel.show_topic(i, topn=10)] for i in range(0, ldamodel.num_topics)}\n",
    "    \n",
    "    return my_dict\n",
    "\n",
    "\n",
    "def get_sentences(doc):\n",
    "    \"\"\"\n",
    "    Taking in a Spacy doc object, returns all sentences of the doc as a list.\n",
    "    \"\"\"\n",
    "    return doc.sents\n",
    "\n",
    "\n",
    "def sentence_sentiment_from_doc(doc):\n",
    "    \"\"\"\n",
    "    Taking in a Spacy doc object, return a list of tuples of all sentences in the doc and their associated sentiment\n",
    "    value (for that sentence) in the form (sentence, sentiment value).\n",
    "    \"\"\"\n",
    "    sentences = get_sentences(doc)\n",
    "    tuple_list = []\n",
    "    for sentence in sentences:\n",
    "        sent_doc = nlp(sentence.text)\n",
    "        tuple_list.append((sentence.text,sent_doc._.blob.polarity)) #list of tuples of form [(text, sentiment)]\n",
    "    return tuple_list\n",
    "\n",
    "\n",
    "def sentence_sentiment_on_topics(doc, topic_list):\n",
    "    \"\"\"\n",
    "    Taking in a Spacy doc object and a generated topic word dictionary (from create_topic_words_dict()), this iterates\n",
    "    through every topic in the dictionary and goes through every word of every topic. For each word, it finds all occurences\n",
    "    of the word in the doc and the local sentiment scores for that word. Then, when we have all sentiment values\n",
    "    for all words in a topic, averages out the sentiment score for that topic and adds it to a dictionary. Finally,\n",
    "    we return the dictionary of all topics and their averaged sentiment scores for each of those topics for that single\n",
    "    article.\n",
    "    \"\"\"\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc) #get all sentences and their sentiment\n",
    "    score_list = []\n",
    "    return_dict = {}\n",
    "    \n",
    "    for key in topic_list: #for every topic\n",
    "        for word in topic_list[key]: #for every word in that topic\n",
    "            for sentence, sentiment in sentence_sentiment_list:\n",
    "                 if sentence.find(word) != -1: #if the word is in that sentence we add the sentiment value\n",
    "                        score_list.append(sentiment)\n",
    "        if not score_list:\n",
    "            return_dict[key] = 0\n",
    "        else:\n",
    "            return_dict[key] = sum(score_list) / len(score_list) #average of all sentence sentiments for topic\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def topic_sentence_sentiment_analysis(df, LDA_model, corpus):\n",
    "    \"\"\"\n",
    "    Takes in our main sentiment analysis dataframe, our LDA model, and the LDA model corpus. This uses all of the functions\n",
    "    above it to generate a dataframe of all articles (rows) and all topics (columns) where the cell corresponds to the \n",
    "    localized sentiment value for that topic on that article.\n",
    "\n",
    "    This is done by gathering each document text and making it a Spacy doc object, then performing our localized sentiment\n",
    "    analysis function on that article and adding that to a larger dictionary of all articles. This will return a dictionary\n",
    "    of dictionaries, where each key is an article URL and the value is a dictionary of all topics and their sentiment values\n",
    "    for that article.\n",
    "    \"\"\"\n",
    "    topicSentDic = {}\n",
    "    topic_list = create_topic_words_dict(LDA_model) #list of topics and their words\n",
    "    \n",
    "    for x in range(len(df[\"URL\"])): #for every article\n",
    "        page_text = df.iloc[x][\"Text\"]\n",
    "        tempdoc = nlp(page_text) #gather page text and transform into doc object\n",
    "        \n",
    "        temp = sentence_sentiment_on_topics(tempdoc,topic_list) #dictionary of all topics and their average sentiment for the article\n",
    "        topicSentDic[x] = temp #append sentiment dict\n",
    "    \n",
    "    return topicSentDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sentiment_per_doc(word, text):\n",
    "    \"\"\"\n",
    "    DEPRECATED FUNCTION (not in use). Takes in a word (string) and a Spacy doc object, and get the localized sentiment\n",
    "    value for that word in the document. Similar to our sentence_sentiment_on_topics() function but only does it for\n",
    "    one word.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc) # get all sentences and their sentiment\n",
    "    word_score = 0\n",
    "    total_appearences = 0\n",
    "    \n",
    "    for sentence, sentiment in sentence_sentiment_list:\n",
    "        if sentence.find(word) != -1: #if the word is in that sentence we add the sentiment value\n",
    "            word_score += sentiment \n",
    "            total_appearences += 1\n",
    "    \n",
    "    if total_appearences == 0:\n",
    "        return None\n",
    "    \n",
    "    word_sentiment = word_score / total_appearences\n",
    "    return word_sentiment\n",
    "\n",
    "def topic_sentiment_per_doc(topics, text):\n",
    "    \"\"\"\n",
    "    DEPRECATED FUNCTION (not in use). Takes in a list of relevant topics and the text of an article, and returns\n",
    "    the local sentiment score for the topic on that article (using a weighted score).\n",
    "    \"\"\"\n",
    "    topic_sentiment_df = []\n",
    "    \n",
    "    for i, topic_tuple in enumerate(topics):\n",
    "        topic_id, topic = topic_tuple\n",
    "        # For each topic\n",
    "        weighted_topic_sentiment = 0\n",
    "        for word, score in topic:\n",
    "            # For each word in a topic\n",
    "            # Multiply the relavence by the sentiment to get a weighted sentiment\n",
    "            word_sentiment = word_sentiment_per_doc(word, text)\n",
    "            if word_sentiment != None:\n",
    "                weighted_word_sentiment = score * word_sentiment\n",
    "                weighted_topic_sentiment += weighted_word_sentiment\n",
    "        topic_sentiment_df.append((topic_id, weighted_topic_sentiment))\n",
    "        \n",
    "    return topic_sentiment_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
