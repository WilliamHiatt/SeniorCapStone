{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0139d3de",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "This file is used for performing sentiment analysis, both on an article as a whole, and on sentences that contain the main topic(s) related to that article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f617f9",
   "metadata": {},
   "source": [
    "Spacy: Used for NLP and has the machine learning module\n",
    "\n",
    "SpacyTextBlob: Used for the sentiment analysis\n",
    "\n",
    "Pandas: Stores the data as a dataframe table\n",
    "\n",
    "NewsPaper: Used for web scraping\n",
    "\n",
    "Requests: Makes the connection to the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pandas as pd\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248255e",
   "metadata": {},
   "source": [
    "# Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5cd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_sentiment_analysis(urls):\n",
    "    # Loops through our URLS and scraps the data\n",
    "    # Put all empty dictionaries here\n",
    "    sentimentDic = {}\n",
    "\n",
    "    for count, x in enumerate(urls):\n",
    "        if(count % 10 == 0): #layman's way of showing progress\n",
    "            print(str(count))\n",
    "\n",
    "        url = x #the url of the article we want to webscrape and analyze\n",
    "\n",
    "        # Send the URL to get scraped, returning the text of the article\n",
    "        page_text = scrapeData(x)\n",
    "\n",
    "        # Runs sentiment analysis. Will need to make a new function and a new dictionary\n",
    "        # for each type of analysis we want to run. Will pass in the page_text, the dic, and\n",
    "        # x (the url)\n",
    "        sentimentDic = sentimentAnalysis(page_text, sentimentDic, url)\n",
    "\n",
    "\n",
    "    # For each analysis we run we need to then convert that dictionary with the following method\n",
    "    df = dictionaryToDataFrame(sentimentDic)\n",
    "    \n",
    "    #Clean dataframe by dropping all rows that failed webscraping\n",
    "    df = drop_failed_webscraping_rows(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab46cb1",
   "metadata": {},
   "source": [
    "# Article Level Sentiment Analysis\n",
    "This code block is used to performe sentiment analysis on the entire document. Analysis is done with TextBlob with the help of Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(text, dictionary, url):\n",
    "    if(len(dictionary) == 0):\n",
    "        dictionary = {\n",
    "            \"URL\": [],\n",
    "            \"Sentiment Score\": [],\n",
    "            \"Sentiment Label\": [],\n",
    "            \"Subjectivity Score\": [],\n",
    "            \"Positive Words\": [],\n",
    "            \"Negative Words\": [],\n",
    "            \"Text\": []\n",
    "            }\n",
    "        \n",
    "    # If there was an error while parsing the document we will not do any sentiment analysis\n",
    "    # on the article text.\n",
    "    if(text[0:8] != \"PARERROR\"):\n",
    "        # Start the sentiment analysis now\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Get's sentiment and subjectivity\n",
    "        sentiment = doc._.blob.polarity\n",
    "        sentiment = round(sentiment,2)\n",
    "        subjectivity = doc._.blob.subjectivity\n",
    "        subjectivity = round(subjectivity,2)\n",
    "\n",
    "        # Gives positive or negative label\n",
    "        if sentiment >= 0.033 and sentiment <= 0.043:\n",
    "            sent_label = \"Neutral\"\n",
    "        elif sentiment > 0.043 and sentiment < 0.143:\n",
    "            sent_label = \"Neutral Positive\"\n",
    "        elif sentiment > 0.143:\n",
    "            sent_label = \"Positive\"\n",
    "        elif sentiment < 0.033 and sentiment > -0.062:\n",
    "            sent_label = \"Neutral Negative\"\n",
    "        elif sentiment < -0.062:\n",
    "            sent_label = \"Negative\"\n",
    "    \n",
    "        # Appending labels to the dictionary\n",
    "        dictionary[\"Sentiment Label\"].append(sent_label)\n",
    "        dictionary[\"Sentiment Score\"].append(sentiment)\n",
    "        dictionary[\"Subjectivity Score\"].append(subjectivity)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "    \n",
    "        # Creating a list of positive and negative words\n",
    "        for x in doc._.blob.sentiment_assessments.assessments:\n",
    "          if x[1] > 0:\n",
    "            positive_words.append(x[0][0])\n",
    "          elif x[1] < 0:\n",
    "            negative_words.append(x[0][0])\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "    \n",
    "    # Hits if there was a scrapping error\n",
    "    else:\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        dictionary[\"Sentiment Label\"].append(text)\n",
    "        dictionary[\"Sentiment Score\"].append(0.0)\n",
    "        dictionary[\"Subjectivity Score\"].append(0.0)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3cf0fc",
   "metadata": {},
   "source": [
    "# Topic Level Sentiment\n",
    "This code block is used to preform the sentiment analysis based on the topic word(s) of an article. It will perform sentiment on the sentences that contain the topic(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#returns a dictionary of all topics, with all their associated topic words in the form {Topic: [words]}\n",
    "def create_topic_words_dict(ldamodel):\n",
    "    my_dict = {'Topic_' + str(i): [token for token, score in ldamodel.show_topic(i, topn=10)] for i in range(0, ldamodel.num_topics)}\n",
    "    \n",
    "    return my_dict\n",
    "\n",
    "#returns all sentences in a document as a list\n",
    "def get_sentences(doc):\n",
    "    return doc.sents\n",
    "\n",
    "#Takes a doc object from spacy and returns a tuple list of form (sentence, sentiment of sentence) for all sentences\n",
    "def sentence_sentiment_from_doc(doc):\n",
    "    sentences = get_sentences(doc)\n",
    "    tuple_list = []\n",
    "    for sentence in sentences:\n",
    "        sent_doc = nlp(sentence.text)\n",
    "        tuple_list.append((sentence.text,sent_doc._.blob.polarity)) #list of tuples of form [(text, sentiment)]\n",
    "    return tuple_list\n",
    "\n",
    "#Returns an average sentiment score of all topics for a single document\n",
    "def sentence_sentiment_on_topics(doc, topic_list):\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc) #get all sentences and their sentiment\n",
    "    score_list = []\n",
    "    return_dict = {}\n",
    "    \n",
    "    for key in topic_list: #for every topic\n",
    "        for word in topic_list[key]: #for every word in that topic\n",
    "            for sentence, sentiment in sentence_sentiment_list:\n",
    "                 if sentence.find(word) != -1: #if the word is in that sentence we add the sentiment value\n",
    "                        score_list.append(sentiment)\n",
    "        if not score_list:\n",
    "            return_dict[key] = 0\n",
    "        else:\n",
    "            return_dict[key] = sum(score_list) / len(score_list) #average of all sentence sentiments for topic\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "def topic_sentence_sentiment_analysis(df, LDA_model, corpus):\n",
    "    #cleaneddf = drop_failed_webscraping_rows(df)\n",
    "    #LDA_model, corpus = create_lda_model(cleaneddf, 20, 5, 5)\n",
    "\n",
    "    topicSentDic = {}\n",
    "    for x in range(len(df[\"URL\"])): #for every article\n",
    "        page_text = df.iloc[x][\"Text\"]\n",
    "        tempdoc = nlp(page_text) #gather page text and transform into doc object\n",
    "        topic_list = create_topic_words_dict(LDA_model) #list of topics and their words\n",
    "        temp = sentence_sentiment_on_topics(tempdoc,topic_list) #dictionary of all topics and their average sentiment for the article\n",
    "        topicSentDic[df.iloc[x][\"URL\"]] = temp #append sentiment dict\n",
    "    \n",
    "    return topicSentDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in a word and a document and get back the average sentiment of that word for the document.\n",
    "def word_sentiment_per_doc(word, text):\n",
    "    doc = nlp(text)\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc) # get all sentences and their sentiment\n",
    "    word_score = 0\n",
    "    total_appearences = 0\n",
    "    \n",
    "    for sentence, sentiment in sentence_sentiment_list:\n",
    "        if sentence.find(word) != -1: #if the word is in that sentence we add the sentiment value\n",
    "            word_score += sentiment \n",
    "            total_appearences += 1\n",
    "    \n",
    "    if total_appearences == 0:\n",
    "        return None\n",
    "    \n",
    "    word_sentiment = word_score / total_appearences\n",
    "    return word_sentiment\n",
    "\n",
    "# takes the relavent topics with the text for the document\n",
    "def topic_sentiment_per_doc(topics, text):\n",
    "    topic_sentiment_df = []\n",
    "    \n",
    "    for i, topic_tuple in enumerate(topics):\n",
    "        topic_id, topic = topic_tuple\n",
    "        # For each topic\n",
    "        weighted_topic_sentiment = 0\n",
    "        for word, score in topic:\n",
    "            # For each word in a topic\n",
    "            # Multiply the relavence by the sentiment to get a weighted sentiment\n",
    "            word_sentiment = word_sentiment_per_doc(word, text)\n",
    "            if word_sentiment != None:\n",
    "                weighted_word_sentiment = score * word_sentiment\n",
    "                weighted_topic_sentiment += weighted_word_sentiment\n",
    "        topic_sentiment_df.append((topic_id, weighted_topic_sentiment))\n",
    "        \n",
    "    return topic_sentiment_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
