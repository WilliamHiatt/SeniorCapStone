{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb0c361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pandas as pd\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea50e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentAnalysis(text, dictionary, url):\n",
    "    if(len(dictionary) == 0):\n",
    "        dictionary = {\n",
    "            \"URL\": [],\n",
    "            \"Sentiment Score\": [],\n",
    "            \"Sentiment Label\": [],\n",
    "            \"Subjectivity Score\": [],\n",
    "            \"Positive Words\": [],\n",
    "            \"Negative Words\": [],\n",
    "            \"Text\": []\n",
    "            }\n",
    "    if(text[0:8] != \"PARERROR\"):\n",
    "        # Start the sentiment analysis now\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        doc = nlp(text)\n",
    "        sentiment = doc._.blob.polarity\n",
    "        sentiment = round(sentiment,2)\n",
    "        subjectivity = doc._.blob.subjectivity\n",
    "        subjectivity = round(subjectivity,2)\n",
    "\n",
    "        # Gives positive or negative label\n",
    "        if sentiment >= 0.033 and sentiment <= 0.043:\n",
    "            sent_label = \"Neutral\"\n",
    "        elif sentiment > 0.043 and sentiment < 0.143:\n",
    "            sent_label = \"Neutral Positive\"\n",
    "        elif sentiment > 0.143:\n",
    "            sent_label = \"Positive\"\n",
    "        elif sentiment < 0.033 and sentiment > -0.062:\n",
    "            sent_label = \"Neutral Negative\"\n",
    "        elif sentiment < -0.062:\n",
    "            sent_label = \"Negative\"\n",
    "\n",
    "        dictionary[\"Sentiment Label\"].append(sent_label)\n",
    "        dictionary[\"Sentiment Score\"].append(sentiment)\n",
    "        dictionary[\"Subjectivity Score\"].append(subjectivity)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "\n",
    "        for x in doc._.blob.sentiment_assessments.assessments:\n",
    "          if x[1] > 0:\n",
    "            positive_words.append(x[0][0])\n",
    "          elif x[1] < 0:\n",
    "            negative_words.append(x[0][0])\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "    \n",
    "    else:\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        dictionary[\"Sentiment Label\"].append(text)\n",
    "        dictionary[\"Sentiment Score\"].append(0.0)\n",
    "        dictionary[\"Subjectivity Score\"].append(0.0)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3cf0fc",
   "metadata": {},
   "source": [
    "# Topic Level Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a58880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sentences(doc):\n",
    "#     return doc.sents\n",
    "\n",
    "# def get_sentence_list(doc):\n",
    "#     return [sent for sent in doc.sents]\n",
    "\n",
    "# #Takes a doc object from spacy and returns a tuple list of form (sentence, sentiment of sentence) for all sentences\n",
    "# def sentence_sentiment_from_doc(doc):\n",
    "#     sentences = get_sentences(doc)\n",
    "#     tuple_list = []\n",
    "#     for sentence in sentences:\n",
    "#         sent_doc = nlp(sentence.text)\n",
    "#         tuple_list.append((sentence.text,sent_doc._.blob.polarity))\n",
    "#     return tuple_list\n",
    "\n",
    "# #takes in a single topic word, the word's weight, and the doc, and returns sentiment of that word within the doc\n",
    "# def sentence_level_sentiment_of_word(word, weight, doc):\n",
    "#     sentence_sentiment_list = sentence_sentiment_from_doc(doc)\n",
    "#     sentiment_total = 0\n",
    "    \n",
    "#     for (sentence, sentiment) in sentence_sentiment_list:\n",
    "#              if sentence.find(word) != -1:\n",
    "#                     #print(name,sentence)\n",
    "#                     sentiment_total += sentiment\n",
    "#     return sentiment_total\n",
    "\n",
    "# def topic_level_sentiment(ldamodel):\n",
    "#     my_dict = {'Topic_' + str(i): [token for token, score in ldamodel.show_topic(i, topn=10)] for i in range(0, ldamodel.num_topics)}\n",
    "    \n",
    "#     return my_dict\n",
    "\n",
    "#returns a dictionary of all topics, with all their associated topic words in the form {Topic: [words]}\n",
    "def create_topic_words_dict(ldamodel):\n",
    "    my_dict = {'Topic_' + str(i): [token for token, score in ldamodel.show_topic(i, topn=10)] for i in range(0, ldamodel.num_topics)}\n",
    "    \n",
    "    return my_dict\n",
    "\n",
    "#returns all sentences in a document as a list\n",
    "def get_sentences(doc):\n",
    "    return doc.sents\n",
    "\n",
    "#Takes a doc object from spacy and returns a tuple list of form (sentence, sentiment of sentence) for all sentences\n",
    "def sentence_sentiment_from_doc(doc):\n",
    "    sentences = get_sentences(doc)\n",
    "    tuple_list = []\n",
    "    for sentence in sentences:\n",
    "        sent_doc = nlp(sentence.text)\n",
    "        tuple_list.append((sentence.text,sent_doc._.blob.polarity)) #list of tuples of form [(text, sentiment)]\n",
    "    return tuple_list\n",
    "\n",
    "#Returns an average sentiment score of all topics for a single document\n",
    "def sentence_sentiment_on_topics(doc, topic_list, df_topics):\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc) #get all sentences and their sentiment\n",
    "    score_list = []\n",
    "    return_dict = {}\n",
    "    \n",
    "    for key in topic_list: #for every topic\n",
    "        for topic in df_topics: # every topic within our current article\n",
    "            print(key[-1])\n",
    "            print(topic[0])\n",
    "            print(type(key[-1]))\n",
    "            print(type(topic[0]))\n",
    "            if int(key[-1]) == topic[0]: # Only does sentiment on topics that are part of the related topics\n",
    "                print(\"HERE\")\n",
    "                for word in topic_list[key]: #for every word in that topic\n",
    "                    for sentence, sentiment in sentence_sentiment_list:\n",
    "                         if sentence.find(word) != -1: #if the word is in that sentence we add the sentiment value\n",
    "                                score_list.append(sentiment)\n",
    "                if not score_list:\n",
    "                    return_dict[key] = 0\n",
    "                else:\n",
    "                    return_dict[key] = sum(score_list) / len(score_list) #average of all sentence sentiments for topic\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "def topic_sentence_sentiment_analysis(df, LDA_model, corpus):\n",
    "    #cleaneddf = drop_failed_webscraping_rows(df)\n",
    "    #LDA_model, corpus = create_lda_model(cleaneddf, 20, 5, 5)\n",
    "\n",
    "    topicSentDic = {}\n",
    "    for x in range(len(df[\"URL\"])): #for every article\n",
    "        page_text = df.iloc[x][\"Text\"]\n",
    "        df_topics = df.iloc[x][\"Topics\"]\n",
    "        tempdoc = nlp(page_text) #gather page text and transform into doc object\n",
    "        topic_list = create_topic_words_dict(LDA_model) #list of topics and their words\n",
    "        temp = sentence_sentiment_on_topics(tempdoc,topic_list, df_topics) #dictionary of all topics and their average sentiment for the article\n",
    "        topicSentDic[df.iloc[x][\"URL\"]] = temp #append sentiment dict\n",
    "    \n",
    "    return topicSentDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in a word and a document and get back the average sentiment of that word for the document.\n",
    "def word_sentiment_per_doc(word, sentence_sentiment_list):\n",
    "    word_score = 0\n",
    "    total_appearences = 0\n",
    "    for sentence, sentiment in sentence_sentiment_list:\n",
    "        if sentence.find(word) != -1: #if the word is in that sentence we add the sentiment value\n",
    "            word_score += sentiment \n",
    "            total_appearences += 1\n",
    "    if total_appearences == 0:\n",
    "        return None\n",
    "    word_sentiment = word_score / total_appearences\n",
    "    return word_sentiment\n",
    "\n",
    "def topic_sentiment_per_doc(topic_words, text):\n",
    "    weighted_topic_sentiment = 0\n",
    "    for word, score in topic_words:\n",
    "        # Get list of sentences whithin the document\n",
    "        doc = nlp(text)\n",
    "        sentence_sentiment_list = sentence_sentiment_from_doc(doc) # get all sentences and their sentiment\n",
    "        # For each word in a topic\n",
    "        # Multiply the relavence by the sentiment to get a weighted sentiment\n",
    "        word_sentiment = word_sentiment_per_doc(word, sentence_sentiment_list)\n",
    "        if word_sentiment != None:\n",
    "            weighted_word_sentiment = score * word_sentiment\n",
    "            weighted_topic_sentiment += weighted_word_sentiment\n",
    "    return weighted_topic_sentiment\n",
    "\n",
    "# The return value is a {Topic: [(document, sentiment), (document, sentiment), ...], Topic: [(document, sentiment), (document, sentiment), ...], ...}\n",
    "\n",
    "# Sentiment per topic per document\n",
    "def topics_per_article(corpus, ldamodel):\n",
    "    topics_list = defaultdict(list)\n",
    "    for doc_id, doc_bow in enumerate(corpus):\n",
    "        doc_topics = ldamodel.get_document_topics(doc_bow)\n",
    "        for topic_id, score in doc_topics:\n",
    "            topics_list[doc_id].append((topic_id, score))\n",
    "    return topics_list\n",
    "\n",
    "def add_sentiment(df, LDA_model, rawData):\n",
    "    # Create a new DataFrame to store the updated values\n",
    "    data = []\n",
    "    \n",
    "    for document_id in df.index:\n",
    "        row = []\n",
    "        for topic_id in df.columns:\n",
    "            topic_words = LDA_model.show_topic(topic_id[0])\n",
    "            current_value = df.at[document_id, topic_id]\n",
    "            new_value = [current_value, topic_sentiment_per_doc(topic_words, rawData[document_id])]\n",
    "            row.append(new_value)\n",
    "            print(((int(topic_id[0]) / df.shape[1]) + int(document_id)) / df.shape[0])\n",
    "        data.append(row)\n",
    "\n",
    "    new_df = pd.DataFrame(data, columns=[i for i in range(df.shape[1])])\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10263d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
