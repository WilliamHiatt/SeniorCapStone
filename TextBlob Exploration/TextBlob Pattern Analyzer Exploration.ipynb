{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d33596b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabri\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba57bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.taggers import PatternTagger\n",
    "from textblob.sentiments import PatternAnalyzer\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a1be300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify the sentiment value into positive, neutral, and negative\n",
    "analyzer = PatternAnalyzer() #POLARITY AND SUBJECTIVITY ANALYZER\n",
    "tagger = PatternTagger() #PART OF SPEECH TAGGER\n",
    "\n",
    "\n",
    "def pattern_analyzer_classify(sentence):\n",
    "    score = analyzer.analyze(sentence)\n",
    "    if score[0] > 0.1:\n",
    "        return \"pos\"\n",
    "    elif score[0] < 0.1:\n",
    "        return \"neg\"\n",
    "    else:\n",
    "        return \"neu\"\n",
    "    \n",
    "#compare results of data using pattern analyzer compared to correct annotations\n",
    "def pattern_analyzer_compare(data, annotations): #compares how many we correctly classify\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    sentence_list = []\n",
    "    for i in data['body-paragraphs']:\n",
    "        for j in i:\n",
    "            sentence_list.append(j)\n",
    "    for phrase in annotations[\"phrase-level-annotations\"]:\n",
    "        if(phrase[\"id\"] == \"title\"):#line below is where we input our model\n",
    "            if (pattern_analyzer_classify(data[\"title\"]) == phrase[\"polarity\"]):\n",
    "                num_correct += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            sentence_id = int(phrase[\"id\"][1:])#input model below here too\n",
    "            if (pattern_analyzer_classify(sentence_list[sentence_id]) == phrase[\"polarity\"]):\n",
    "                num_correct += 1\n",
    "            total += 1\n",
    "    return total, num_correct\n",
    "\n",
    "###TAGGER FUNCTIONS\n",
    "#POS tagging list of tags https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/\n",
    "\n",
    "def pattern_tag(sentence):\n",
    "    blob = TextBlob(sentence, pos_tagger=tagger)\n",
    "    return blob.pos_tags\n",
    "\n",
    "def find_proper_names(pos_tags):\n",
    "    tuple_list = []\n",
    "    for pos_tuple in pos_tags:\n",
    "        #find the NNP (Proper nouns) tagged words and return those tuples as a new list\n",
    "        if pos_tuple[1] == 'NNP':\n",
    "            tuple_list.append(pos_tuple)\n",
    "    return tuple_list\n",
    "\n",
    "def find_full_proper_names_list(pos_tags): #only accounts for first+last names\n",
    "    name_list = []\n",
    "    prev_tuple = (\"___\",'___')\n",
    "    for pos_tuple in pos_tags:\n",
    "        #find the NNP tagged words and return those tuples as a new list\n",
    "        if pos_tuple[1] == 'NNP':\n",
    "            if prev_tuple[1] == 'NNP':\n",
    "                name_list.remove(prev_tuple[0])\n",
    "                name_list.append(prev_tuple[0] + \" \" + pos_tuple[0])\n",
    "            else:\n",
    "                name_list.append(pos_tuple[0])\n",
    "        prev_tuple = pos_tuple\n",
    "    return name_list\n",
    "\n",
    "def name_analysis(sentence):\n",
    "    p_tags = pattern_tag(sentence) #list of tuple of words and their part of speech\n",
    "    p_names_list = find_proper_names(p_tags) #list of proper nouns\n",
    "    \n",
    "    score_assessments = analyzer.analyze(sentence, keep_assessments = True) #polarity, subjectivity, assessments list\n",
    "    assessments = score_assessments[2] #get only the list of assessed words\n",
    "    \n",
    "    return_list = [] # will be a list of names and their assessed polarity scores\n",
    "    \n",
    "    assessed_words = [] #list of just the words that are assessed\n",
    "    for word in assessments:\n",
    "        #print(word)\n",
    "        assessed_words.append(word[0][0])\n",
    "    \n",
    "#     assessed_words_and_assessments = []\n",
    "#     for word in assessments:\n",
    "#         #print(word)\n",
    "#         assessed_words.append((word[0][0],word[1]))\n",
    "    \n",
    "    for word_tuple in p_tags:\n",
    "        if word_tuple in p_names_list: #if we find a proper name we calculate the sentiment around it\n",
    "            #print(word_tuple) \n",
    "            \n",
    "            name_total_assessment_score = 0 #the total assessment for a single name\n",
    "            \n",
    "            name_index = p_tags.index(word_tuple)\n",
    "            \n",
    "            start_index = 0\n",
    "            end_index = 0\n",
    "            \n",
    "            if name_index - 3 > 0: #we create a word cluster 3 before and 3 after the name\n",
    "                start_index = name_index - 3\n",
    "                \n",
    "            if name_index + 4 > len(p_tags):\n",
    "                end_index = len(p_tags)\n",
    "            else:\n",
    "                end_index = name_index + 4\n",
    "            #print(assessments)\n",
    "            #print(p_tags[start_index:end_index][0])\n",
    "            \n",
    "            counter = 0 # need to replace counter with any tuple in range, right now it only checks one at a time\n",
    "            for assessment in assessments: #iterate through assessed words tuple\n",
    "                #print(assessment)\n",
    "                #print(p_tags[start_index:end_index][:])\n",
    "                for i in range(start_index,end_index): #for every assessment, iterate through words within range\n",
    "                    #we check if this assessment's word matches one in the list\n",
    "                    if assessment[0][0] in p_tags[i][0].lower(): #CHECK THIs should check if assessed word is in range\n",
    "                        name_total_assessment_score += assessment[1]\n",
    "                    counter += 1\n",
    "                \n",
    "            return_list.append((word_tuple[0],name_total_assessment_score))\n",
    "    print(return_list)\n",
    "    return return_list\n",
    "#             for adj_word in p_tags[start_index:end_index]: #here we iterate over all of the adjacent words and find if they have been assessed\n",
    "#                 #print(adj_word)\n",
    "#                 if adj_word[0].lower() in assessed_words:\n",
    "#                     list = \n",
    "            #I need to get assessments values within the range\n",
    "            #then add them and add to a list in form (proper_noun,assessment_sum)\n",
    "    \n",
    "    \n",
    "#TEXTBLOB MANIPULATION FUNCTIONS\n",
    "\n",
    "def textblob_to_sentences_list(tb):\n",
    "    sent_list = []\n",
    "    for sentence in tb.sentences:\n",
    "        sent_list.append(str(sentence))\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d90e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Donald', -0.4), ('Trump', -0.4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Donald', -0.4), ('Trump', -0.4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###EXPLORATION START\n",
    "pattern_analyzer1 = PatternAnalyzer()\n",
    "\n",
    "test_sent1 = \"I hate the things that Trump has done over the years.\"\n",
    "test_sent2 = \"In an Epic Battle of Tanks, Russia Was Routed, Repeating Earlier Mistakes\"\n",
    "test_sent3 = \"Ukraine war live updates: Russian mercenary boss says ‘fierce resistance’ seen in Bakhmut; Kyiv says its fighters are under ‘insane pressure’\"\n",
    "test_sent4 = \"What’s Worse for Donald Trump Than Getting Indicted?\"\n",
    "test_sent5 = \"A fierce debate between Obama and Donald Trump ensued on tuesday.\"\n",
    "#pattern analyzer returns a tuple of polarity, subjectivity\n",
    "#and if you set it to true also a list of assessments made on \n",
    "#individual words\n",
    "\n",
    "#basic sentence analyzer tests\n",
    "# print(\"Pattern analyzer test1:\")\n",
    "# print(pattern_analyzer1.analyze(test_sent1)) #polarity and subjectivity\n",
    "\n",
    "# test_analyzing2 = pattern_analyzer1.analyze(test_sent2,keep_assessments = True)\n",
    "# print(test_analyzing2) #analyze with assessments of major words\n",
    "\n",
    "# print(\"Pattern test3: \" + pattern_analyzer_classify(test_sent3))\n",
    "# print(pattern_analyzer1.analyze(test_sent3))\n",
    "\n",
    "# textblob1 = TextBlob(\"Russia returns the broken fighter jet remnants to the US in exchange for political prisoners.\")\n",
    "# tb1_sent_list = textblob_to_sentences_list(textblob1)\n",
    "# for sentence in tb1_sent_list:\n",
    "#     name_analysis(sentence)\n",
    "\n",
    "#print(pattern_analyzer1.analyze(test_sent4, keep_assessments = True))    \n",
    "name_analysis(test_sent4)    \n",
    "# print(pattern_analyzer1.analyze(test_sent1)[0])\n",
    "\n",
    "\n",
    "\n",
    "# #TAGGING EXPLORATION\n",
    "# tags1 = pattern_tag(test_sent1)\n",
    "# print(tags1)\n",
    "# print(\"\\n\")\n",
    "# print(find_proper_names(tags1))\n",
    "# print(find_full_proper_names_list(tags1))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# tags4 = pattern_tag(test_sent4)\n",
    "# print(tags4)\n",
    "# print(\"\\n\")\n",
    "# print(find_proper_names(tags4))\n",
    "# print(find_full_proper_names_list(tags4))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# tags5 = pattern_tag(test_sent5)\n",
    "# print(tags5)\n",
    "# print(find_full_proper_names_list(tags5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c66fa4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern sentiment on BASIL correct: 1130.000 , BASIL total: 1726.000\n",
      "Pattern sentiment on BASIL ACCURACY: 0.6547\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "\n",
    "#open BASIL dataset and gather the articles w/o scores\n",
    "for i in range(10):\n",
    "    file_i = os.listdir(\"BASILdata/articles/201\" + str(i))\n",
    "    for file_name in file_i:\n",
    "        file = open(\"BASILdata/articles/201\" + str(i) + \"/\" + file_name, encoding=\"utf8\")\n",
    "        json_file = json.load(file)\n",
    "        file_list.append(json_file)\n",
    "        \n",
    "annotation_file_list = []\n",
    "\n",
    "#open BASIL dataset and gather annotations for articles\n",
    "for i in range(10):\n",
    "    file_i = os.listdir(\"BASILdata/annotations/201\" + str(i))\n",
    "    for file_name in file_i:\n",
    "        file = open(\"BASILdata/annotations/201\" + str(i) + \"/\" + file_name, encoding=\"utf8\")\n",
    "        json_file = json.load(file)\n",
    "        annotation_file_list.append(json_file)\n",
    "\n",
    "total_correct = 0\n",
    "total = 0\n",
    "\n",
    "#compare correct annotations to the results from our model\n",
    "for i in range(len(file_list)):\n",
    "    data = file_list[i]\n",
    "    annotations = annotation_file_list[i]\n",
    "    a, b = pattern_analyzer_compare(data, annotations)\n",
    "    total += a\n",
    "    total_correct += b\n",
    "    \n",
    "accuracy = total_correct/total\n",
    "print(\"Pattern sentiment on BASIL correct: %.3f , BASIL total: %.3f\" %(total_correct, total))\n",
    "print(\"Pattern sentiment on BASIL ACCURACY: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2244f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
